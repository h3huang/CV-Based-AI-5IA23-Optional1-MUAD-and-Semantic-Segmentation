{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lNWDh5S8zjw"
   },
   "source": [
    "In a new python environment with python>=3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4sDlD3KX8zjy",
    "outputId": "5f4fe9e3-346d-4db9-fc08-712db73871bb"
   },
   "outputs": [],
   "source": [
    "!pip install \"torch_uncertainty[image] @ git+https://github.com/ENSTA-U2IS-AI/torch-uncertainty@dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dcLeda8S8zjz"
   },
   "outputs": [],
   "source": [
    "# here are the training parameters\n",
    "batch_size = 10\n",
    "learning_rate =1e-3\n",
    "weight_decay=2e-4\n",
    "lr_decay_epochs=20\n",
    "lr_decay=0.1\n",
    "nb_epochs=50\n",
    "# Skip training and load model locally.\n",
    "# If never trained, set to False to train and save the model first.\n",
    "skip_training = True # True to skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "56e074b8d0504d13a05c724c6e673fea",
      "10941100a4a04060b5891a1ec79647e9",
      "174715c81639432e8e33783ded405959",
      "b60446792cb447318e4dc26316e15410",
      "622c5291b5674ea7a95e588e8c722b52",
      "2610c4af815f4c4caefdc4a7a8cf199c",
      "40d03656a3ec47e4be10f002f6ad9922",
      "54760cf043a740f6855ab4d8f56b9601",
      "089c32d9a0b54369b8b6536223e68195",
      "bd76cf707f374a438dc4dc8d3c12af25",
      "42804acd881449c1b35ad4ef6f94ad96",
      "041d46ad3ecf41e6ad86405c84084e73",
      "6f44db9362e949b2be6520f84b84e2d9",
      "82262684bfe049e58d385bfdbc1c4b2f",
      "f2989c0c7e274460993dd1051a66bb8b",
      "8fbfe482410545a4bb9d7ccd8db6cd4b",
      "494608918dbf445daff7c6abb2d95de2",
      "21f0616157cf4de994ca1aa8391fcfd1",
      "e747833e09394fe6902db7df47fe5b38",
      "2e840a2b3fe140fc9e5d82fd6ca5f67e",
      "574c44c1e6554652a4058315b55895d3",
      "c0edd880e22b49d2b4ff0672f196bf92",
      "ee312c05cbc3436ba1938d5b350f1c23",
      "8ca6dde86b5a4cac82cbd3eeb1e011cd",
      "ee6d8de55c6f4780b0a12feeeb866a80",
      "0cd9325b2f9e44489b5dfbba26d09453",
      "61a6e4810ba74f9cbb978526f2875076",
      "be3ff9fcc2c940b88fb33bf3b37c5c80",
      "3a00c7af3eb94dfcb3f3475379483e2e",
      "8b30130488fe493a9d77253291f16851",
      "54f548cf599d47b4b4220825588de5ef",
      "6a9a2f2025da4db7a6a8c1ca495bd915",
      "c792a283f71e419a8243c17cdb5b7693"
     ]
    },
    "id": "6TqJpiUX8zjz",
    "outputId": "a4db431c-eab7-47ab-996e-a13844197a6a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "\n",
    "from torch_uncertainty.datasets import MUAD\n",
    "\n",
    "\n",
    "import os\n",
    "#My personal token\n",
    "os.environ[\"HF_TOKEN\"] = \"HF_TOKEN_PLACEHOLDER\"\n",
    "\n",
    "train_transform = v2.Compose(\n",
    "    [\n",
    "        v2.Resize(size=(256, 512), antialias=True),\n",
    "        v2.RandomHorizontalFlip(),\n",
    "        v2.ToDtype(\n",
    "            dtype={\n",
    "                tv_tensors.Image: torch.float32,\n",
    "                tv_tensors.Mask: torch.int64,\n",
    "                \"others\": None,\n",
    "            },\n",
    "            scale=True,\n",
    "        ),\n",
    "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = v2.Compose(\n",
    "    [\n",
    "        v2.Resize(size=(256, 512), antialias=True),\n",
    "        v2.ToDtype(\n",
    "            dtype={\n",
    "                tv_tensors.Image: torch.float32,\n",
    "                tv_tensors.Mask: torch.int64,\n",
    "                \"others\": None,\n",
    "            },\n",
    "            scale=True,\n",
    "        ),\n",
    "        v2.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_set = MUAD(root=\"./data\", target_type=\"semantic\", version=\"small\", split=\"train\" , transforms=train_transform, download=True)\n",
    "val_set = MUAD(root=\"./data\", target_type=\"semantic\", version=\"small\", split=\"val\" , transforms=val_transform, download=True)\n",
    "test_set = MUAD(root=\"./data\", target_type=\"semantic\", version=\"small\", split=\"test\" , transforms=val_transform, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9qKz4D38zj0"
   },
   "source": [
    "Let us see the first sample of the validation set. The first image is the input and the second image is the target (ground truth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3rgOIk4B8zj0",
    "outputId": "e28ad4b7-319d-4442-d44c-a01f4db57f7c"
   },
   "outputs": [],
   "source": [
    "sample = train_set[0]\n",
    "img, tgt = sample\n",
    "img.size(), tgt.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8mlTc2u8zj1"
   },
   "source": [
    "Visualize a validation input sample (and RGB image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "P5EF4vBD8zj1",
    "outputId": "52aa4205-771c-4acd-cd9b-05b677a3ce1a"
   },
   "outputs": [],
   "source": [
    "# Undo normalization on the image and convert to uint8.\n",
    "mean = torch.tensor([0.485, 0.456, 0.406], device=img.device)\n",
    "std = torch.tensor([0.229, 0.224, 0.225], device=img.device)\n",
    "img = img * std[:, None, None] + mean[:, None, None]\n",
    "img = F.to_dtype(img, torch.uint8, scale=True)\n",
    "F.to_pil_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyKaF3mu8zj1"
   },
   "source": [
    "Visualize the same image above but segmented (our goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "YgTqq-hN8zj2",
    "outputId": "a218701c-9811-440c-9430-acf40d88ca20"
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import draw_segmentation_masks\n",
    "\n",
    "tmp_tgt = tgt.masked_fill(tgt == 255, 21)\n",
    "tgt_masks = tmp_tgt == torch.arange(22, device=tgt.device)[:, None, None]\n",
    "img_segmented = draw_segmentation_masks(img, tgt_masks, alpha=1, colors=val_set.color_palette)\n",
    "F.to_pil_image(img_segmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K21284qq8zj2"
   },
   "source": [
    "Below is the complete list of classes in MUAD, presented as:\n",
    "\n",
    "1.   Class Name\n",
    "2.   Train ID\n",
    "3.   Segmentation Color in RGB format [R,G, B]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5CBuq3FL8zj3",
    "outputId": "6875d127-4c5b-43f9-fea0-21a788f7ed53"
   },
   "outputs": [],
   "source": [
    "for muad_class in train_set.classes:\n",
    "    class_name = muad_class.name\n",
    "    train_id = muad_class.id\n",
    "    color = muad_class.color\n",
    "    print(f\"Class: {class_name}, Train ID: {train_id}, Color: {color}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xM6QQ8GP8zj3"
   },
   "source": [
    "Here is a more comprhensive review of the diffrent classes : (while training Non-labeled data will use train ID 21 and not 255)\n",
    "\n",
    "\n",
    "| **class names**                       | **ID** |\n",
    "|----------------------------------------|---------|\n",
    "| road                                   | 0       |\n",
    "| sidewalk                               | 1       |\n",
    "| building                               | 2       |\n",
    "| wall                                   | 3       |\n",
    "| fence                                  | 4       |\n",
    "| pole                                   | 5       |\n",
    "| traffic light                          | 6       |\n",
    "| traffic sign                           | 7       |\n",
    "| vegetation                             | 8       |\n",
    "| terrain                                | 9       |\n",
    "| sky                                    | 10      |\n",
    "| person                                 | 11      |\n",
    "| rider                                  | 12      |\n",
    "| car                                    | 13      |\n",
    "| truck                                  | 14      |\n",
    "| bus                                    | 15      |\n",
    "| train                                  | 16      |\n",
    "| motorcycle                             | 17      |\n",
    "| bicycle                                | 18      |\n",
    "| bear deer cow                          | 19      |\n",
    "| garbage_bag stand_food trash_can       | 20      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IBM8fzf8zj3"
   },
   "source": [
    "We will feed our DNN the first raw image of the road view and as target it will be the dark image below and not the colored one (second image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "wU4A6w428zj3",
    "outputId": "16960bb7-a345-4a06-afa3-b4493e867a4b"
   },
   "outputs": [],
   "source": [
    "im = F.to_pil_image(F.to_dtype(tgt, torch.uint8))\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hLRSaJgV8zj3",
    "outputId": "90e5cde2-aa3b-459c-a263-a659274aa197"
   },
   "outputs": [],
   "source": [
    "im.size\n",
    "print(np.array(im))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdF1nx6T8zj4"
   },
   "source": [
    "**Why is the target image dark and what's the bright part ?** **(hint : print the numpy array)**\n",
    "\n",
    "A: The most part of the image is dark because the value of each pixel (elements in the printed array, e.g., 2, 8, 0, and so on) represents the class ID as the table shown before. Since the pixel range is $[0,255]$ where 0 represents black and 255 represents white, we see most part of the image is black with the fact that most items are recognized by the model, while some of them are not and labeled with class ID 255, being white in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXbrvwC98zj4"
   },
   "source": [
    "**Q3/ please study the dataset a bit. What it is about?**\n",
    "\n",
    "A3: MUAD is a uncertainty benchmark dataset for various tasks in autonomous driving. According to the configuration, we are now using the small version of MUAD, with a target of semantics (instead of depth). The corresponding ground truth information each sample in the dataset includes mainly semantic segmentation and the depth map, and the configuration shows our goal of semantic segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ivjRh97e8zj4",
    "outputId": "1e57a301-3b7d-4788-8130-712ed720f817"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "        train_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "        val_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "        test_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zn9_b_KT8zj4"
   },
   "outputs": [],
   "source": [
    "def enet_weighing(dataloader, num_classes, c=1.02):\n",
    "    \"\"\"Computes class weights as described in the ENet paper.\n",
    "\n",
    "        w_class = 1 / (ln(c + p_class)),\n",
    "\n",
    "    where c is usually 1.02 and p_class is the propensity score of that\n",
    "    class:\n",
    "\n",
    "        propensity_score = freq_class / total_pixels.\n",
    "\n",
    "    References:\n",
    "        https://arxiv.org/abs/1606.02147\n",
    "\n",
    "    Args:\n",
    "        dataloader (``data.Dataloader``): A data loader to iterate over the\n",
    "            dataset.\n",
    "        num_classes (``int``): The number of classes.\n",
    "        c (``int``, optional): AN additional hyper-parameter which restricts\n",
    "            the interval of values for the weights. Default: 1.02.\n",
    "\n",
    "    \"\"\"\n",
    "    class_count = 0\n",
    "    total = 0\n",
    "    for _, label in dataloader:\n",
    "      label = label.cpu().numpy()\n",
    "      # Flatten label\n",
    "      flat_label = label.flatten()\n",
    "      flat_label = flat_label[flat_label != 255]\n",
    "\n",
    "      # Sum up the number of pixels of each class and the total pixel\n",
    "      # counts for each label\n",
    "      class_count += np.bincount(flat_label, minlength=num_classes)\n",
    "      total += flat_label.size\n",
    "\n",
    "    # Compute propensity score and then the weights for each class\n",
    "    propensity_score = class_count / total\n",
    "    return 1 / (np.log(c + propensity_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HfrMULrG8zj4",
    "outputId": "f3d48a0b-458a-402d-88b9-c7d37392de95"
   },
   "outputs": [],
   "source": [
    "print(\"\\nComputing class weights...\")\n",
    "print(\"(this can take a while depending on the dataset size)\")\n",
    "class_weights = enet_weighing(train_loader, 19)\n",
    "class_weights = torch.from_numpy(class_weights).float().cuda()\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcyhmI8m8zj5"
   },
   "source": [
    "**Q4/ why do we need to evaluate the class_weights?**\n",
    "\n",
    "A4: The sample number of each class is different (i.e., the dataset itself is not balanced). This means potential big differences among the weights of classes, which leads to the case that the model is biased to the majority classes with many samples in the dataset and ignore the minority. We introduce a custom class weighing scheme from ENet to restrict the weights in a certain range, so as for weighting the loss function during training to signal to the model that it should pay greater attention to samples from minority classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tkKjz_k8zj5"
   },
   "source": [
    "## C. building the DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g46XoY1P8zj5"
   },
   "source": [
    "**Q5/ Do we really use Unet? What did I change :)? (that is hard)**\n",
    "\n",
    "A5: Yes, but a little bit different from the original Unet. There are no dropout layers in the decoder part of the original one. You insert dropout layers for uncertainty evaluation.\n",
    "\n",
    "\n",
    "\n",
    "**Q6/Do we need a backbone with Unet?**\n",
    "\n",
    "A6: No, we don't, at least not necessary. The Unet is a complete encoder-decoder architecture which can complete the end-to-end problem, i.e., takes as input the images and outputs the segmentation map. Though we can load pretrained networks like resnets as the backbone, or encoder, to enhance the performance, that is not necessary.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dn_vEBFM8zj5"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(conv => BN => ReLU) * 2.\"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class InConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_ch, out_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mpconv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super().__init__()\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.up = nn.ConvTranspose2d(in_ch // 2, in_ch // 2, 2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        if self.bilinear:\n",
    "            x1 = F.resize(x1, size=[2*x1.size()[2],2*x1.size()[3]],\n",
    "                          interpolation=v2.InterpolationMode.BILINEAR)\n",
    "        else:\n",
    "            x1 = self.up(x1)\n",
    "\n",
    "        # input is CHW\n",
    "        diff_y = x2.size()[2] - x1.size()[2]\n",
    "        diff_x = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diff_x // 2, diff_x - diff_x // 2,\n",
    "                        diff_y // 2, diff_y - diff_y // 2])\n",
    "\n",
    "        # for padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "#please note that we have added dropout layer to be abble to use MC dropout\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, classes):\n",
    "        super().__init__()\n",
    "        self.inc = InConv(3, 32)\n",
    "        self.down1 = Down(32, 64)\n",
    "        self.down2 = Down(64, 128)\n",
    "        self.down3 = Down(128, 256)\n",
    "        self.down4 = Down(256, 256)\n",
    "        self.up1 = Up(512, 128)\n",
    "        self.up2 = Up(256, 64)\n",
    "        self.up3 = Up(128, 32)\n",
    "        self.up4 = Up(64, 32)\n",
    "        self.dropout = nn.Dropout2d(0.1)\n",
    "        self.outc = OutConv(32, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.dropout(x)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.dropout(x)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.dropout(x)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.dropout(x)\n",
    "        return self.outc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9xW-iRN8zj6"
   },
   "source": [
    "## D. Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4n2-8wIB8zj6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Colors from Colorbrewer Paired_12\n",
    "colors = [[31, 120, 180], [51, 160, 44]]\n",
    "colors = [(r / 255, g / 255, b / 255) for (r, g, b) in colors]\n",
    "\n",
    "def plot_losses(train_history, val_history):\n",
    "    x = np.arange(1, len(train_history) + 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(x, train_history, color=colors[0], label=\"Training loss\", linewidth=2)\n",
    "    plt.plot(x, val_history, color=colors[1], label=\"Validation loss\", linewidth=2)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(\"Evolution of the training and validation loss\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_accu(train_history, val_history):\n",
    "    x = np.arange(1, len(train_history) + 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(x, train_history, color=colors[0], label=\"Training miou\", linewidth=2)\n",
    "    plt.plot(x, val_history, color=colors[1], label=\"Validation miou\", linewidth=2)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean IoU\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(\"Evolution of Miou\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0gLFWkM8zj7"
   },
   "source": [
    "**Q7/  what is the IoU?**\n",
    "\n",
    "A7: IoU, Intersection over Union, is defined as:\n",
    "$$\n",
    "\n",
    "IoU = \\frac{\\text{area of overlap}}{\\text{area of union}}\n",
    "\n",
    "$$\n",
    "This is a metric to evaluate the accuracy of the model, which preliminarily satisfies the requirement for calculating the geometric similarity between two images, providing a straightforward implementation of image overlap measurement. However, it fails to account for the distance between the two shapes or the similarity in their aspect ratios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWD37D48zj7"
   },
   "source": [
    "### Training function\n",
    "\n",
    "**Q8/Please complete the training and the test function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ETHIIe68zj7"
   },
   "outputs": [],
   "source": [
    "from torchmetrics.utilities.compute import _safe_divide\n",
    "\n",
    "\n",
    "def train( model, data_loader, optim, criterion, metric,iteration_loss=False):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    metric.reset()\n",
    "    for step, batch_data in enumerate(data_loader):\n",
    "        # Get the inputs and labels\n",
    "        img = batch_data[0].cuda()\n",
    "        labels = batch_data[1].cuda()\n",
    "        labels[labels >= 19] = 255\n",
    "        # Squeeze the channel dimension if present: [B, 1, H, W] -> [B, H, W]\n",
    "        if labels.ndim == 4 and labels.shape[1] == 1:\n",
    "            labels = labels.squeeze(1)\n",
    "        # Forward propagation\n",
    "        outputs = model(img)\n",
    "        \n",
    "        # Loss computation\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "\n",
    "        # Flatten the outputs and labels for metric computation\n",
    "        flatten_logits = rearrange(outputs, \"b c h w -> (b h w) c\")\n",
    "        flatten_labels = labels.flatten()\n",
    "        valid_mask = flatten_labels != 255\n",
    "        \n",
    "        # Keep track of loss for current epoch\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Keep track of the evaluation metric\n",
    "        metric.update(flatten_logits[valid_mask].detach(), flatten_labels[valid_mask].detach())\n",
    "\n",
    "        if iteration_loss:\n",
    "            print(\"[Step: %d] Iteration loss: %.4f\" % (step, loss.item()))\n",
    "\n",
    "    # Compute IoU per class\n",
    "    tp, fp, _, fn = metric._final_state()\n",
    "    iou_per_class = _safe_divide(tp, tp + fp + fn, zero_division=float(\"nan\"))\n",
    "\n",
    "    return epoch_loss / len(data_loader), iou_per_class, metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GiMNyCRY8zj8"
   },
   "source": [
    "### Validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_UTKVIP8zj8"
   },
   "outputs": [],
   "source": [
    "def test(model, data_loader, criterion, metric, iteration_loss=False):\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    metric.reset()\n",
    "    for step, batch_data in enumerate(data_loader):\n",
    "        # Get the inputs and labels\n",
    "        img = batch_data[0].cuda()\n",
    "        labels = batch_data[1].cuda()\n",
    "        labels[labels >= 19] = 255\n",
    "        # Squeeze the channel dimension if present: [B, 1, H, W] -> [B, H, W]\n",
    "        if labels.ndim == 4 and labels.shape[1] == 1:\n",
    "            labels = labels.squeeze(1)\n",
    "        with torch.no_grad():\n",
    "            # Forward propagation\n",
    "            outputs = model(img)\n",
    "            \n",
    "            # Flatten the outputs and labels for metric computation\n",
    "            flatten_logits = rearrange(outputs, \"b c h w -> (b h w) c\")\n",
    "            flatten_labels = labels.flatten()\n",
    "            valid_mask = flatten_labels != 255\n",
    "\n",
    "            # Loss computation\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # Keep track of loss for current epoch\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Keep track of evaluation the metric\n",
    "        metric.update(flatten_logits[valid_mask], flatten_labels[valid_mask])\n",
    "\n",
    "        if iteration_loss:\n",
    "            print(\"[Step: %d] Iteration loss: %.4f\" % (step, loss.item()))\n",
    "\n",
    "    # Compute IoU per class\n",
    "    tp, fp, _, fn = metric._final_state()\n",
    "    iou_per_class = _safe_divide(tp, tp + fp + fn, zero_division=float(\"nan\"))\n",
    "\n",
    "    return epoch_loss / len(data_loader), iou_per_class, metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiwqqSLU8zj8"
   },
   "source": [
    "## E. Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzAQQAS98zj9"
   },
   "source": [
    "**Q9/ please train your DNN and comment?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p3QBxf8N8zj9",
    "outputId": "be18de69-4abf-4842-9e8e-ae89fda49bb0"
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torch_uncertainty.metrics.segmentation import MeanIntersectionOverUnion\n",
    "\n",
    "print(\"\\nTraining...\\n\")\n",
    "num_classes = 19\n",
    "# Intialize UNet\n",
    "model = UNet(num_classes)\n",
    "model = model.cuda()\n",
    "\n",
    "# We are going to use the CrossEntropyLoss loss function as it's most\n",
    "# frequentely used in classification problems with multiple classes which\n",
    "# fits the problem. This criterion  combines LogSoftMax and NLLLoss.\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights,ignore_index=255)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=nb_epochs)\n",
    "print(\"\\nRe-computing class weights for 19 classes...\")\n",
    "full_weights = enet_weighing(train_loader, 21) \n",
    "class_weights = torch.from_numpy(full_weights[:19]).float().cuda() # 只取前19个\n",
    "\n",
    "print(\"Class weights shape:\", class_weights.shape)\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vWqD3h2Q8zj9",
    "outputId": "429a13ed-a8ab-402a-dfd6-cfbb29771375"
   },
   "outputs": [],
   "source": [
    "# Start Training\n",
    "# Training loop\n",
    "train_losses = []\n",
    "train_IoU = []\n",
    "test_losses = []\n",
    "test_IoU = []\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "best_iou = 0.0\n",
    "# Initialize metric objects globally for the loop (optional, but cleaner if reset inside)\n",
    "metric_train_obj = MeanIntersectionOverUnion(num_classes=num_classes).to(device)\n",
    "metric_test_obj = MeanIntersectionOverUnion(num_classes=num_classes).to(device)\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{nb_epochs}\")\n",
    "    \n",
    "    # Train and Validate\n",
    "    # Note: train/test functions return (loss, per_class_iou, mean_iou) based on your implementation\n",
    "    train_loss, IoU_per_class_train, _ = train(model, train_loader, optimizer, criterion, metric_train_obj)\n",
    "    test_loss, IoU_per_class_test, _ = test(model, test_loader, criterion, metric_test_obj)\n",
    "    \n",
    "    # Scheduler step\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Record history\n",
    "    train_losses.append(train_loss)\n",
    "    # Calculate mean of per-class IoU manually or use the 3rd return value from your function\n",
    "    train_IoU.append(IoU_per_class_train.nanmean().item()) \n",
    "    \n",
    "    test_losses.append(test_loss)\n",
    "    test_miou = IoU_per_class_test.nanmean().item()\n",
    "    test_IoU.append(test_miou)\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train mIoU: {train_IoU[-1]:.4f}\")\n",
    "    print(f\"  Test Loss:  {test_loss:.4f} | Test mIoU:  {test_miou:.4f}\")\n",
    "\n",
    "    # Save best model logic\n",
    "    if test_miou > best_iou:\n",
    "        best_iou = test_miou\n",
    "        torch.save(model.state_dict(), 'unet_best.pth')\n",
    "        print(\"  -> Best model saved!\")\n",
    "\n",
    "# Save final model as well\n",
    "torch.save(model.state_dict(), 'unet.pth')\n",
    "\n",
    "# Plotting ONCE at the end\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss', color=colors[0])\n",
    "plt.plot(test_losses, label='Test Loss', color=colors[1])\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_IoU, label='Train IoU', color=colors[0])\n",
    "plt.plot(test_IoU, label='Test IoU', color=colors[1])\n",
    "plt.title('mIoU over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean IoU')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_curves_unet.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pOhpPHK8zj-"
   },
   "source": [
    "Load a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N8E3A5gi8zj-",
    "outputId": "0db62f0f-438f-49ca-e200-d4043594e5b9"
   },
   "outputs": [],
   "source": [
    "#Loading a model\n",
    "model = UNet(19)\n",
    "model.load_state_dict(torch.load(\"unet.pth\"))\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsDiU4G_8zj-"
   },
   "source": [
    "# III. Evalution of the Trained DNN on the test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djpEkMSF8zkB"
   },
   "source": [
    "## A. classical evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cv5mjGFp8zkC"
   },
   "source": [
    "**Q10/ please plot the loss and miou and comment about it ?**\n",
    "Both the training and test loss curves show a similar downward trend, stabilizing around 0.3. However, there's a notable difference in the mean Intersection over Union (mIoU) values: the training mIoU reaches approximately 0.75, whereas the test mIoU plateaus at around 0.65. This discrepancy indicates a definite overfitting to the training dataset.\n",
    "\n",
    "A10: The curve is similar to the description, we note that the model is potentially overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "N11rmcci8zkC",
    "outputId": "59100ab6-e7dd-4b26-f4b1-10b21dbc3240"
   },
   "outputs": [],
   "source": [
    "plot_losses(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "qV-GoYE78zkC",
    "outputId": "1b1b7327-d6b8-459b-8c0f-464690fe2784"
   },
   "outputs": [],
   "source": [
    "plot_accu(train_IoU, test_IoU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfAjk5J98zkC"
   },
   "source": [
    "**Q11/ what should we have done to avoid overfitting?**\n",
    "\n",
    "A11: We have introduced dropout layers in the decoder part of Unet. To further avoid overfitting, we can:\n",
    "\n",
    "1. Increase the dropout values (e.g., 0.2 to 0.3)\n",
    "2. Augment the dataset with random erasing, cropping, erasing, and so on, in order to force the model to learn local features of an image.\n",
    "3. Use an optimizer with weight decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CyOU5dc18zkD",
    "outputId": "c05129fa-b0d3-4a3a-dc08-79b8d3756d60"
   },
   "outputs": [],
   "source": [
    "# Now we evaluate the model on all the test set.\n",
    "loss, iou, miou = test(model, test_loader, criterion, metric_test_obj)\n",
    "print(\">>>> [FINAL TEST on the test set: ] Avg. loss: \", loss ,\" | Mean IoU: \", miou)\n",
    "class_names = [c.name for c in test_set.classes if c.id < 19]\n",
    "# Print per class IoU on last epoch or if best iou\n",
    "for key, class_iou in zip(class_names, iou, strict=True):\n",
    "  print(f\"{key}: {class_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7gPJw158zkD"
   },
   "source": [
    "## B. Uncertainty evaluations with MCP\n",
    "Here you will just use as confidence score the Maximum class probability (MCP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0L4fmecp8zkD"
   },
   "outputs": [],
   "source": [
    "sample_idx = 0\n",
    "img, target = test_set[sample_idx]\n",
    "\n",
    "batch_img = img.unsqueeze(0).cuda()\n",
    "batch_target = target.unsqueeze(0).cuda()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\t# Forward propagation\n",
    "\toutputs = model(batch_img)\n",
    "\toutputs_proba = outputs.softmax(dim=1)\n",
    "\t# remove the batch dimension\n",
    "\toutputs_proba = outputs_proba.squeeze(0)\n",
    "\tconfidence, pred = outputs_proba.max(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "afn94s1m8zkD",
    "outputId": "2e932552-8577-45d5-94e6-2676c3d20cdf"
   },
   "outputs": [],
   "source": [
    "# Undo normalization on the image and convert to uint8.\n",
    "mean = torch.tensor([0.485, 0.456, 0.406], device=img.device)\n",
    "std = torch.tensor([0.229, 0.224, 0.225], device=img.device)\n",
    "img = img * std[:, None, None] + mean[:, None, None]\n",
    "img = F.to_dtype(img, torch.uint8, scale=True)\n",
    "\n",
    "tmp_target = target.masked_fill(target == 255, 21)\n",
    "target_masks = tmp_target == torch.arange(22, device=target.device)[:, None, None]\n",
    "img_segmented = draw_segmentation_masks(img, target_masks, alpha=1, colors=test_set.color_palette)\n",
    "\n",
    "pred_masks = pred == torch.arange(22, device=pred.device)[:, None, None]\n",
    "\n",
    "pred_img = draw_segmentation_masks(img, pred_masks, alpha=1, colors=test_set.color_palette)\n",
    "\n",
    "img = F.to_pil_image(img)\n",
    "img_segmented = F.to_pil_image(img_segmented)\n",
    "confidence_img = F.to_pil_image(confidence)\n",
    "pred_img = F.to_pil_image(pred_img)\n",
    "\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(30, 15))\n",
    "ax1.imshow(img)\n",
    "ax2.imshow(img_segmented)\n",
    "ax3.imshow(pred_img)\n",
    "ax4.imshow(confidence_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTLLCpTQ8zkD"
   },
   "source": [
    "**Q12/ The last image is the related to the confidence score of the DNN. Can you explain why? What does the birght areas represent and what does the dark areas represent?**\n",
    "\n",
    "Because the image visualizes the maximum class probability, a proxy for the model's confidence. Bright and yellow areas represent high confidence, meaning that the model is confident that its prediction in these areas. The dark areas represent low confidence, and the model represents uncertainty here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2lBaYGD8zkD"
   },
   "source": [
    "### Now let's load the OOD test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188,
     "referenced_widgets": [
      "3aeabb14ca274d5586626ad3810ff8be",
      "d9592877be8340e9972cc5e64771d9e5",
      "e8f6dae37e9d45dd85e59bf7ae1f4475",
      "ad1c3b194cec48c59f6d3ba63264ea0b",
      "464bf93f3d1a4393be814192c8e525d8",
      "bb41c87913bf455dbb5427a3a19e49fd",
      "23cabf10a47c421a82101e4e608c52dc",
      "e7545048af6045909544e8a67035df55",
      "0269ce8d19c24193ad272c0adfa732dc",
      "d17dc53149c34717973758cc21f38a66",
      "580cc34e31bb445ebaee7725e1ed05fc"
     ]
    },
    "id": "1EK6l_yi8zkE",
    "outputId": "ab08eea7-274d-4ba3-8fcd-bc01c23ef27a"
   },
   "outputs": [],
   "source": [
    "test_ood_set = MUAD(root=\"./data\", target_type=\"semantic\", version=\"small\", split=\"ood\" , transforms=val_transform, download=True)\n",
    "test_ood_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oonIF23g8zkE"
   },
   "outputs": [],
   "source": [
    "sample_idx = 0\n",
    "img, target = test_ood_set[sample_idx]\n",
    "\n",
    "batch_img = img.unsqueeze(0).cuda()\n",
    "batch_target = target.unsqueeze(0).cuda()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\t# Forward propagation\n",
    "\toutputs = model(batch_img)\n",
    "\toutputs_proba = outputs.softmax(dim=1)\n",
    "\t# remove the batch dimension\n",
    "\toutputs_proba = outputs_proba.squeeze(0)\n",
    "\tconfidence, pred = outputs_proba.max(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "ikr2WKe98zkE",
    "outputId": "6a5d70c6-9fb9-4e5e-93d0-fa87ee04aaa0"
   },
   "outputs": [],
   "source": [
    "# Undo normalization on the image and convert to uint8.\n",
    "mean = torch.tensor([0.485, 0.456, 0.406], device=img.device)\n",
    "std = torch.tensor([0.229, 0.224, 0.225], device=img.device)\n",
    "img = img * std[:, None, None] + mean[:, None, None]\n",
    "img = F.to_dtype(img, torch.uint8, scale=True)\n",
    "\n",
    "tmp_target = target.masked_fill(target == 255, 21)\n",
    "target_masks = tmp_target == torch.arange(22, device=target.device)[:, None, None]\n",
    "img_segmented = draw_segmentation_masks(img, target_masks, alpha=1, colors=test_set.color_palette)\n",
    "\n",
    "pred_masks = pred == torch.arange(22, device=pred.device)[:, None, None]\n",
    "\n",
    "pred_img = draw_segmentation_masks(img, pred_masks, alpha=1, colors=test_set.color_palette)\n",
    "\n",
    "img_pil = F.to_pil_image(img)\n",
    "img_segmented = F.to_pil_image(img_segmented)\n",
    "confidence_img = F.to_pil_image(confidence)\n",
    "pred_img = F.to_pil_image(pred_img)\n",
    "\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(30, 15))\n",
    "ax1.imshow(img_pil)\n",
    "ax2.imshow(img_segmented)\n",
    "ax3.imshow(pred_img)\n",
    "ax4.imshow(confidence_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOhtqIgm8zkE"
   },
   "source": [
    "**According to the output is the model confident when it comes to labeling the bear and goat ? How about the bench ?**\n",
    "\n",
    "A: The model is over-confident when it comes to labeling the bear, the goat, and the bench. We train the model under the case that we ignore classes with ID greater than 19, such that those classes can be used as OOD objects. In the last image, the areas corresponding to the bear and the goat, and the bench are darker than the surrounding yellow marked items that the model is confident about, but not dark enough. They are still quite bright, i.e., the model is over-confident about these OOD objects, though the model doesn't know what they are. This means the model incorrectly classifies them as something In-Distribution. In specific, the bear in the 3rd image is marked in blue, which means it is classified as a car, and similarly the goat is classified as a pedestrian, the bench is divided into several parts corresponding to sidewalk, fence, and pedestrian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fvvIDQd8zkE"
   },
   "source": [
    "\n",
    "**Q12 bis/ The last image is the related to the confidence score of the DNN. Can you explain why?**\n",
    "**Are you happy with this image?**\n",
    "\n",
    "Because the image visualizes the maximum class probability, a proxy for the model's confidence. As we mentioned before, bright and yellow areas represent high confidence, meaning that the model is confident that its prediction in these areas. The dark areas represent low confidence, and the model represents uncertainty here.\n",
    "\n",
    "We are not happy with this image, because the model is over-confident and tends to classify OOD objects as ID objects. Ideally, for those OOD objects, corresponding areas in the confidence map should be dark or blue, however, the fact that it is bright yellow indicates that the model is poorly calibrated and fails to flag these anomalies as unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLGo9SKQ8zkE"
   },
   "source": [
    "## C. Uncertainty evaluations with Temperature Scaling\n",
    "**Q13/ please implement a temperature scaling using torch_uncertainty**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4R2towX48zkF"
   },
   "source": [
    "Before Temprature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wmxLKFIO8zkF",
    "outputId": "1db66d39-9ab8-420c-84ae-6f54252a77e1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch_uncertainty.post_processing import TemperatureScaler\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Extract Logits and Targets from Validation Set\n",
    "# -------------------------------------------------------------------\n",
    "logits_list = []\n",
    "targets_list = []\n",
    "model.eval()\n",
    "\n",
    "print(\"Extracting validation logits and targets for calibration...\")\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        logits = model(images)\n",
    "        logits_list.append(logits)\n",
    "        targets_list.append(labels)\n",
    "\n",
    "# Concatenate all batches\n",
    "# val_logits: (N, C, H, W)\n",
    "# val_targets: (N, 1, H, W) or (N, H, W)\n",
    "val_logits = torch.cat(logits_list)\n",
    "val_targets = torch.cat(targets_list)\n",
    "\n",
    "# Remove channel dim from targets if present: (N, 1, H, W) -> (N, H, W)\n",
    "if val_targets.dim() == 4 and val_targets.shape[1] == 1:\n",
    "    val_targets = val_targets.squeeze(1)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Filter Ignore Index (255) & Flatten\n",
    "# -------------------------------------------------------------------\n",
    "# Move channels to last dim for flattening: (N, H, W, C)\n",
    "val_logits = val_logits.permute(0, 2, 3, 1)\n",
    "\n",
    "# Flatten to pixel-wise: (N*H*W, C) and (N*H*W,)\n",
    "val_logits_flat = val_logits.reshape(-1, val_logits.shape[-1])\n",
    "val_targets_flat = val_targets.reshape(-1)\n",
    "\n",
    "# Create mask for valid pixels (not 255)\n",
    "# Note: Ensure targets are Long type\n",
    "val_targets_flat = val_targets_flat.long()\n",
    "mask = val_targets_flat != 255\n",
    "\n",
    "# Apply mask\n",
    "valid_logits = val_logits_flat[mask]\n",
    "valid_targets = val_targets_flat[mask]\n",
    "\n",
    "print(f\"Original pixels: {val_targets_flat.shape[0]}, Valid pixels: {valid_targets.shape[0]}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Prepare Calibration DataLoader (Pixel-wise)\n",
    "# -------------------------------------------------------------------\n",
    "# We use a large batch size for scalar fitting since it's just efficient computation\n",
    "calibration_dataset = TensorDataset(valid_logits, valid_targets)\n",
    "calibration_loader = DataLoader(calibration_dataset, batch_size=4096, shuffle=True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. Initialize and Fit TemperatureScaler\n",
    "# -------------------------------------------------------------------\n",
    "scaler = TemperatureScaler(init_val=1.0)\n",
    "scaler = scaler.cuda()\n",
    "\n",
    "print(\"Fitting TemperatureScaler...\")\n",
    "scaler.fit(calibration_loader)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. Create Calibrated Model Wrapper and Evaluate\n",
    "# -------------------------------------------------------------------\n",
    "class ModelWithTemperature(torch.nn.Module):\n",
    "    def __init__(self, model, scaler):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.scaler = scaler\n",
    "    def forward(self, input):\n",
    "        return self.scaler(self.model(input))\n",
    "\n",
    "calibrated_model = ModelWithTemperature(model, scaler)\n",
    "\n",
    "print(\"Evaluating calibrated model...\")\n",
    "loss, iou, miou = test(calibrated_model, test_loader, criterion, metric_test_obj)\n",
    "print(f\"After Temperature Scaling - Avg. loss: {loss:.4f} | Mean IoU: {miou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9e8GJiWI8zkF"
   },
   "source": [
    "**Seeing the two graphs above comment on the MCP unceratinty result, is the model overconfident or calibrated ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKXrWIxo8zkF"
   },
   "source": [
    "After temperature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_v_bTeuX8zkF",
    "outputId": "49f0fc80-fd39-489d-ab4d-2f3fbed191d3"
   },
   "outputs": [],
   "source": [
    "from torch_uncertainty.post_processing import TemperatureScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_KLW4uf8zkF"
   },
   "source": [
    "Now let's see the new confidence score image after scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch_uncertainty.metrics.classification import CalibrationError\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(\"Extracting validation logits and targets...\")\n",
    "logits_list = []\n",
    "targets_list = []\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        if labels.ndim == 4:\n",
    "            labels = labels.squeeze(1)\n",
    "            \n",
    "        logits = model(images)\n",
    "        logits_list.append(logits)\n",
    "        targets_list.append(labels)asdfghjklxcvbnm,./+63\n",
    "\t\t\n",
    "\n",
    "val_logits = torch.cat(logits_list).detach()\n",
    "val_targets = torch.cat(targets_list).detach()\n",
    "\n",
    "\n",
    "temperature = nn.Parameter(torch.ones(1).cuda() * 1.5)\n",
    "\n",
    "\n",
    "nll_criterion = nn.CrossEntropyLoss(ignore_index=255) \n",
    "\n",
    "optimizer = optim.LBFGS([temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "def eval_nll():\n",
    "    optimizer.zero_grad()\n",
    "    loss = nll_criterion(val_logits / temperature, val_targets)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "print(f\"Temperature before: {temperature.item():.4f}\")\n",
    "optimizer.step(eval_nll)\n",
    "print(f\"Temperature after:  {temperature.item():.4f}\")\n",
    "\n",
    "\n",
    "class ModelWithTemperature(nn.Module):\n",
    "    def __init__(self, model, temperature):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "    def forward(self, input):\n",
    "        return self.model(input) / self.temperature\n",
    "\n",
    "calibrated_model = ModelWithTemperature(model, temperature)\n",
    "\n",
    "print(\"\\nEvaluating calibrated model...\")\n",
    "loss, iou, miou = test(calibrated_model, test_loader, criterion, metric_test_obj)\n",
    "print(f\"After Scaling - Avg. loss: {loss:.4f} | Mean IoU: {miou:.4f}\")\n",
    "\n",
    "\n",
    "ece_metric = CalibrationError(task=\"multiclass\", num_classes=19, num_bins=15, norm=\"l1\")\n",
    "calibrated_model.eval()\n",
    "all_probs = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        if labels.ndim == 4: labels = labels.squeeze(1) # Fix维度\n",
    "        \n",
    "        logits = calibrated_model(images)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "        \n",
    "        probs = probs.permute(0, 2, 3, 1).reshape(-1, 19)\n",
    "        labels = labels.reshape(-1)\n",
    "        \n",
    "        mask = labels != 255\n",
    "        all_probs.append(probs[mask])\n",
    "        all_targets.append(labels[mask])\n",
    "\n",
    "test_probs = torch.cat(all_probs)\n",
    "test_targets = torch.cat(all_targets)\n",
    "\n",
    "print(f\"Test ECE after Scaling: {ece_metric(test_probs, test_targets).item():.4f}\")\n",
    "\n",
    "try:\n",
    "    ece_metric.plot()\n",
    "    plt.show()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4ZyTEsZ8zkG"
   },
   "source": [
    "**Did the model get more confident ? or is it more calibrated ? Commnet on the temperature scaling graphs and results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDWeuhIh8zkG"
   },
   "source": [
    "## D. Uncertainty evaluations with MC Dropout\n",
    "\n",
    "Let us implement **MC dropout**. This technique decribed in [this paper](https://arxiv.org/abs/1506.02142) allow us to have a better confindence score by using the dropout during test time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCcmyJ0V8zkG"
   },
   "source": [
    "**Q\\14 Please implement MC Dropout using torch_uncertainty**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "RZQO-FDL8zkG",
    "outputId": "b84712a4-be88-4d7c-b001-3f26f3063f41"
   },
   "outputs": [],
   "source": [
    "from torch_uncertainty.models.wrappers.mc_dropout import mc_dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqeweZaq8zkH"
   },
   "source": [
    "**Try the MC dropout code with a low number of estimators T like 3 and a high number 20, Explain the diffrence seen on the confidence image, is the model getting more confident or less ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faf7ifKb8zkH"
   },
   "source": [
    "## E. Uncertainty evaluations with Deep Ensembles\n",
    "**Q\\15 Please implement [Deep Ensembles](https://papers.nips.cc/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf).**\n",
    "\n",
    "\n",
    "1.   You need to train 3 DNNs and save it. (Go back to the training cell above and train and save 3 diffrent models)\n",
    "2.   Use TorchUncertainty to get predictions\n",
    "\n",
    "You have two options either train several models using the code above or use TU to train the ensemble of models in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcUk8mfK8zkH"
   },
   "outputs": [],
   "source": [
    "from torch_uncertainty.models import deep_ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWHyRI0d8zkH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7r8KMm08zkH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QWTayHkn8zkI",
    "outputId": "f544a529-3740-4042-dd57-99a9796c5297"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6VjlLJ08zkI"
   },
   "source": [
    "Test your ensemble obtained either using option 1 or 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FvVP-Xdr8zkI",
    "outputId": "1dd82cb6-d20a-4eef-8b86-bfc125da0a10"
   },
   "outputs": [],
   "source": [
    "results = trainer.test(ens_routine, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8Tc7bav8zkJ"
   },
   "source": [
    "Save the ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uJ9y8Woz8zkJ",
    "outputId": "c8027608-420e-4052-d1bf-f6a683df5282"
   },
   "outputs": [],
   "source": [
    "final_model_path = \"ensemble.pth\"\n",
    "torch.save(ensemble.state_dict(), final_model_path)\n",
    "print(f\"Model saved to {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZf8B6xf8zkJ"
   },
   "source": [
    "## F. Uncertainty evaluations with Packed-Ensembles\n",
    "**Q\\15 Please read [Packed-Ensembles](https://arxiv.org/pdf/2210.09184). Then Implement a Packed-Ensembles Unet and train it and evaluate its Uncertainty**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74wEu5jG8zkJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eh9Uruml8zkJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfKOdgTv8zkK",
    "outputId": "3e6caccc-3062-46ab-a874-51d18a02506b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vERbFaOf8zkK",
    "outputId": "a1a03f52-30d5-4677-8ecf-1a27547ba32f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0txU0p_f8zkK",
    "outputId": "e37ab431-167f-4104-a3d7-eb6a4eadf1d1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czg42bYrRcG7"
   },
   "source": [
    "**Please conclude your report**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "OOD_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0269ce8d19c24193ad272c0adfa732dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "041d46ad3ecf41e6ad86405c84084e73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6f44db9362e949b2be6520f84b84e2d9",
       "IPY_MODEL_82262684bfe049e58d385bfdbc1c4b2f",
       "IPY_MODEL_f2989c0c7e274460993dd1051a66bb8b"
      ],
      "layout": "IPY_MODEL_8fbfe482410545a4bb9d7ccd8db6cd4b"
     }
    },
    "089c32d9a0b54369b8b6536223e68195": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0cd9325b2f9e44489b5dfbba26d09453": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a9a2f2025da4db7a6a8c1ca495bd915",
      "placeholder": "​",
      "style": "IPY_MODEL_c792a283f71e419a8243c17cdb5b7693",
      "value": " 1.06G/1.06G [00:25&lt;00:00, 43.2MB/s]"
     }
    },
    "10941100a4a04060b5891a1ec79647e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2610c4af815f4c4caefdc4a7a8cf199c",
      "placeholder": "​",
      "style": "IPY_MODEL_40d03656a3ec47e4be10f002f6ad9922",
      "value": "train.zip: 100%"
     }
    },
    "174715c81639432e8e33783ded405959": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54760cf043a740f6855ab4d8f56b9601",
      "max": 3827406962,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_089c32d9a0b54369b8b6536223e68195",
      "value": 3827406962
     }
    },
    "21f0616157cf4de994ca1aa8391fcfd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "23cabf10a47c421a82101e4e608c52dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2610c4af815f4c4caefdc4a7a8cf199c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e840a2b3fe140fc9e5d82fd6ca5f67e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3a00c7af3eb94dfcb3f3475379483e2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3aeabb14ca274d5586626ad3810ff8be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d9592877be8340e9972cc5e64771d9e5",
       "IPY_MODEL_e8f6dae37e9d45dd85e59bf7ae1f4475",
       "IPY_MODEL_ad1c3b194cec48c59f6d3ba63264ea0b"
      ],
      "layout": "IPY_MODEL_464bf93f3d1a4393be814192c8e525d8"
     }
    },
    "40d03656a3ec47e4be10f002f6ad9922": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "42804acd881449c1b35ad4ef6f94ad96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "464bf93f3d1a4393be814192c8e525d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "494608918dbf445daff7c6abb2d95de2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54760cf043a740f6855ab4d8f56b9601": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54f548cf599d47b4b4220825588de5ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "56e074b8d0504d13a05c724c6e673fea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_10941100a4a04060b5891a1ec79647e9",
       "IPY_MODEL_174715c81639432e8e33783ded405959",
       "IPY_MODEL_b60446792cb447318e4dc26316e15410"
      ],
      "layout": "IPY_MODEL_622c5291b5674ea7a95e588e8c722b52"
     }
    },
    "574c44c1e6554652a4058315b55895d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "580cc34e31bb445ebaee7725e1ed05fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "61a6e4810ba74f9cbb978526f2875076": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "622c5291b5674ea7a95e588e8c722b52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a9a2f2025da4db7a6a8c1ca495bd915": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f44db9362e949b2be6520f84b84e2d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_494608918dbf445daff7c6abb2d95de2",
      "placeholder": "​",
      "style": "IPY_MODEL_21f0616157cf4de994ca1aa8391fcfd1",
      "value": "val.zip: 100%"
     }
    },
    "82262684bfe049e58d385bfdbc1c4b2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e747833e09394fe6902db7df47fe5b38",
      "max": 511251992,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2e840a2b3fe140fc9e5d82fd6ca5f67e",
      "value": 511251992
     }
    },
    "8b30130488fe493a9d77253291f16851": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ca6dde86b5a4cac82cbd3eeb1e011cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be3ff9fcc2c940b88fb33bf3b37c5c80",
      "placeholder": "​",
      "style": "IPY_MODEL_3a00c7af3eb94dfcb3f3475379483e2e",
      "value": "test.zip: 100%"
     }
    },
    "8fbfe482410545a4bb9d7ccd8db6cd4b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad1c3b194cec48c59f6d3ba63264ea0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d17dc53149c34717973758cc21f38a66",
      "placeholder": "​",
      "style": "IPY_MODEL_580cc34e31bb445ebaee7725e1ed05fc",
      "value": " 193M/193M [00:04&lt;00:00, 42.3MB/s]"
     }
    },
    "b60446792cb447318e4dc26316e15410": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd76cf707f374a438dc4dc8d3c12af25",
      "placeholder": "​",
      "style": "IPY_MODEL_42804acd881449c1b35ad4ef6f94ad96",
      "value": " 3.83G/3.83G [01:36&lt;00:00, 31.8MB/s]"
     }
    },
    "bb41c87913bf455dbb5427a3a19e49fd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd76cf707f374a438dc4dc8d3c12af25": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be3ff9fcc2c940b88fb33bf3b37c5c80": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0edd880e22b49d2b4ff0672f196bf92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c792a283f71e419a8243c17cdb5b7693": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d17dc53149c34717973758cc21f38a66": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9592877be8340e9972cc5e64771d9e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bb41c87913bf455dbb5427a3a19e49fd",
      "placeholder": "​",
      "style": "IPY_MODEL_23cabf10a47c421a82101e4e608c52dc",
      "value": "ood.zip: 100%"
     }
    },
    "e747833e09394fe6902db7df47fe5b38": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7545048af6045909544e8a67035df55": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8f6dae37e9d45dd85e59bf7ae1f4475": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7545048af6045909544e8a67035df55",
      "max": 192729915,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0269ce8d19c24193ad272c0adfa732dc",
      "value": 192729915
     }
    },
    "ee312c05cbc3436ba1938d5b350f1c23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8ca6dde86b5a4cac82cbd3eeb1e011cd",
       "IPY_MODEL_ee6d8de55c6f4780b0a12feeeb866a80",
       "IPY_MODEL_0cd9325b2f9e44489b5dfbba26d09453"
      ],
      "layout": "IPY_MODEL_61a6e4810ba74f9cbb978526f2875076"
     }
    },
    "ee6d8de55c6f4780b0a12feeeb866a80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b30130488fe493a9d77253291f16851",
      "max": 1063029826,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_54f548cf599d47b4b4220825588de5ef",
      "value": 1063029826
     }
    },
    "f2989c0c7e274460993dd1051a66bb8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_574c44c1e6554652a4058315b55895d3",
      "placeholder": "​",
      "style": "IPY_MODEL_c0edd880e22b49d2b4ff0672f196bf92",
      "value": " 511M/511M [00:12&lt;00:00, 41.3MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
